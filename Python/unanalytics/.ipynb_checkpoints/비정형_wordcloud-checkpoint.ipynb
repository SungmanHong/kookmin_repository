{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from konlpy.tag import Kkma # 단어 처리\n",
    "from konlpy.tag import Twitter # 단어 처리\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.font_manager # font 관련\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import lxml.html # url 요청에 대한 html 형식 처리\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word(doc):\n",
    "    '''길이가 2 이상인 단어만 뽑기(글내용)'''\n",
    "    tagger = Twitter()\n",
    "    nouns = tagger.nouns(doc)\n",
    "    \n",
    "    remove_noun = []\n",
    "    with open('remove_word.txt', 'r', newline='\\r\\n', encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            remove_noun.append(line.strip())\n",
    "            \n",
    "    res = []\n",
    "    for noun in nouns:\n",
    "        if ((len(noun) > 1) & (noun not in remove_noun)) :\n",
    "            res.append(noun)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reply_word(doc):\n",
    "    '''길이가 2 이상인 단어만 뽑기(답변)'''\n",
    "    tagger = Twitter()\n",
    "    nouns = tagger.nouns(doc)\n",
    "    \n",
    "#     remove_noun = []\n",
    "#     with open('remove_reply_word.txt', 'r', newline='\\r\\n', encoding='utf8') as f:\n",
    "#         for line in f.readlines():\n",
    "#             remove_noun.append(line.strip())\n",
    "            \n",
    "    res = []\n",
    "    for noun in nouns:\n",
    "#          & (noun not in remove_noun)\n",
    "        if ((len(noun) > 1)) :\n",
    "            res.append(noun)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_content = []\n",
    "reply_content = []\n",
    "\n",
    "with open('content.csv', 'r', newline='\\r\\n', encoding='utf8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        text = re.sub(\"[a-zA-z]|[0-9+]|[(\\d{2})[/.-](\\d{2})[/.-](\\d{4})$]|[*~^.?/(\\)/[\\]/,-:㈜=&>|;”!@<-]|[★■☞☎③②①☏→※▼▣✔￭ⓒ◇●▶]\", \"\", row[0]).replace('%%㎡%%㎡%%㎡%%','').replace('--',' ').replace('㎡','').replace('/','').replace('%','').replace('〔〕','').replace('〔  〕','').replace('(','').replace(')','').replace('──','')\n",
    "#         print(re.sub(\"[a-zA-Z0-9_[]\\xa0\\r\\n\\t::~]%\", \"\", row[0].replace('\\u3000', ' ').replace('\\xa0',' ').replace('\\r\\n', ' ').replace('\\t', '').replace('::', '').replace('~', '').replace('.','').replace('%','').replace('')))\n",
    "        article_content.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reply.csv', 'r', newline='\\r\\n', encoding='utf8') as f2:\n",
    "    reader_reply = csv.reader(f2)\n",
    "    for row in reader_reply:\n",
    "#         print(re.sub(\"[!~^.?><★♡*;ㅜㅠㅎ]\",\"\",row[0]))\n",
    "#         print(row[0])\n",
    "        reply_content.append(re.sub(\"[!~^.?><★♡*;ㅜㅠㅎ]\",\"\",row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_count(keyword, news) :    \n",
    "    '''분석된 단어와 단어의 빈도수 가져오는 함수'''\n",
    "    ### 형태소 분석기\n",
    "    tagger = Twitter()\n",
    "    words=[];count=[];tdf=[]\n",
    "#     cv = CountVectorizer(tokenizer=tagger.nouns, max_features=50)\n",
    "#     tdf = cv.fit_transform(news)\n",
    "    \n",
    "    ### 한단어 빼기 \n",
    "    if (keyword == \"article\"):\n",
    "        cv = CountVectorizer(tokenizer=get_word, max_features=50)\n",
    "        tdf = cv.fit_transform(news)\n",
    "        words = cv.get_feature_names()\n",
    "    else:\n",
    "        cv = CountVectorizer(tokenizer=get_reply_word, max_features=50)\n",
    "        tdf = cv.fit_transform(news)\n",
    "        words = cv.get_feature_names()\n",
    "    print (words)\n",
    "    \n",
    "    count_mat = tdf.sum(axis=0) # 열별로 단어별 출현 빈도 합계 구함(axis = 1 , 각 문서별 명사의 사용 개수)\n",
    "    count = np.squeeze(np.asarray(count_mat)) # 대괄호가 하나로 줄어듬. 좀더 데이터 핸들링을 쉽게 하기 위해 리스트 형태로 해줌\n",
    "    print (count)\n",
    "    \n",
    "    return words, count, tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가능성', '가점', '거주지', '결과', '경쟁률', '계약', '기간', '납입', '답변', '당첨', '도움', '마감', '문의', '미만', '발표', '본인', '분양', '블록', '서류', '세대주', '소득', '소유', '순위', '신규', '신청', '신혼부부', '아파트', '예정', '우선', '월세', '위해', '익산', '인터넷', '일반', '일정', '자격', '자녀', '자동차', '자산', '장기', '재건축', '저축', '전세', '전주', '접수', '주변', '지구', '지역', '질문', '추가']\n",
      "[ 68  47  47  83  76  53  54  48 122 154  45  54  59  43  49  48  58 322\n",
      "  73 155 198  51 289  59 301  77 404  80 203  43  43  45 135 104  74  66\n",
      "  83  54  66 183  64  50 201  43 384  43 230 121  56  44]\n"
     ]
    }
   ],
   "source": [
    "words , count, tdf = words_count(\"article\", article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_reply, count_reply, tdf_reply = words_count(\"reply\", reply_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-59561d891b83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'font'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'HCR Dotum'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# font 지정\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mword_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'word'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'count'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mword_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mword_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "matplotlib.rc('font', family='HCR Dotum') # font 지정\n",
    "word_data = {'word' : words, 'count' : count}\n",
    "word_df = pd.DataFrame(word_data, columns =['word', 'count'] )\n",
    "word_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
