splits <- h2o.splitFrame(highage_36, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
splits <- h2o.splitFrame(highage_36, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_36), response) # 전체컬럼
predictors
m1 <- h2o.deeplearning(   model_id = "dl_model_first",   training_frame = train,   validation_frame = valid, ## validation dataset: used for scoring and early stopping   x = predictors,   y = response,   #activation="Rectifier",  ## default   #hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each   epochs = 1,   variable_importances = T ## not enabled by default )
summary(m1)
pred <- h2o.predict(m1, test)
pred
test$Accuracy <- pred$predict == test$W_4
1 - mean(test$Accuracy)
h2o.removeAll() ## clean slate - just in case the cluster was already running
predictors_constant_36 <- c("REGNUM_BLOCK", " W_34 ", " W_36 ", " W_15 ", " W_38 ", " W_17 ", " W_39 ", " WA_5 ", " W_3 ", " WA_7 ", " W_40 ", " W_30 ", " W_41 ", " WA_1 ", " WA_4 ")
predictors_constant_36
highage_36_remove <- highage_36[, !(names(highage_36) %in% predictors_constant_36)]
highage_36_remove <- highage_36[, !(names(highage_36) %in% predictors_constant_36)]
highage_36 <- read.table('highage_36.csv', header = TRUE)
highage_36 <- as.h2o(highage_36) highage_36
highage_36 <- as.h2o(highage_36)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
rm(list = ls())
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_36 <- read.table('highage_36.csv', header = TRUE)
highage_36 <- as.h2o(highage_36)
highage_36
predictors_constant_36 <- c("REGNUM_BLOCK", " W_34 ", " W_36 ", " W_15 ", " W_38 ", " W_17 ", " W_39 ", " WA_5 ", " W_3 ", " WA_7 ", " W_40 ", " W_30 ", " W_41 ", " WA_1 ", " WA_4 ")
predictors_constant_36
highage_36_remove <- highage_36[, !(names(highage_36) %in% predictors_constant_36)]
highage_36_remove
splits <- h2o.splitFrame(highage_36, c(0.6, 0.2), seed = 1234)
splits <- h2o.splitFrame(predictors_constant_36, c(0.6, 0.2), seed = 1234)
splits <- h2o.splitFrame(predictors_constant_36, c(0.6, 0.2), seed = 1234)
splits <- h2o.splitFrame(highage_36_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼
predictors
m1 <- h2o.deeplearning(   model_id = "dl_model_first",   training_frame = train,   validation_frame = valid, ## validation dataset: used for scoring and early stopping   x = predictors,   y = response,   #activation="Rectifier",  ## default   #hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each   epochs = 1,   variable_importances = T ## not enabled by default )
rm(list = ls())
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_36 <- read.table('highage_36.csv', header = TRUE)
highage_36 <- as.h2o(highage_36)
highage_36
predictors_constant_36 <- c("REGNUM_BLOCK", " W_34 ", " W_36 ", " W_15 ", " W_38 ", " W_17 ", " W_39 ", " WA_5 ", " W_3 ", " WA_7 ", " W_40 ", " W_30 ", " W_41 ", " WA_1 ", " WA_4 ")
predictors_constant_36
highage_36_remove <- highage_36[, !(names(highage_36) %in% predictors_constant_36)]
highage_36_remove
splits <- h2o.splitFrame(highage_36_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼
predictors
m1 <- h2o.deeplearning(   model_id = "dl_model_first",   training_frame = train,   validation_frame = valid, ## validation dataset: used for scoring and early stopping   x = predictors,   y = response,   #activation="Rectifier",  ## default   #hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each   epochs = 1,   variable_importances = T ## not enabled by default )
highage_36_remove
highage_36[, !(names(highage_36) %in% predictors_constant_36)]
predictors_constant_36 <- c("REGNUM_BLOCK", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_36
highage_36_remove <- highage_36[, !(names(highage_36) %in% predictors_constant_36)]
highage_36_remove
splits <- h2o.splitFrame(highage_36_remove, c(0.6, 0.2), seed = 1234) train <- h2o.assign(splits[[1]], "train.hex") # 60% valid <- h2o.assign(splits[[2]], "valid.hex") # 60% test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼 predictors
m1 <- h2o.deeplearning(   model_id = "dl_model_first",   training_frame = train,   validation_frame = valid, ## validation dataset: used for scoring and early stopping   x = predictors,   y = response,   #activation="Rectifier",  ## default   #hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each   epochs = 1,   variable_importances = T ## not enabled by default )
summary(m1)
pred <- h2o.predict(m1, test)
pred
test$Accuracy <- pred$predict == test$W_4
1 - mean(test$Accuracy)
rm(list = ls())
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_46 <- read.table('highage_36.csv', header = TRUE)
highage_46 <- as.h2o(highage_46)
highage_46
splits <- h2o.splitFrame(highage_46, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_46), response) # 전체컬럼
predictors
m1 <- h2o.deeplearning(   model_id = "dl_model_first",   training_frame = train,   validation_frame = valid, ## validation dataset: used for scoring and early stopping   x = predictors,   y = response,   #activation="Rectifier",  ## default   #hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each   epochs = 1,   variable_importances = T ## not enabled by default )
summary(m1)
pred <- h2o.predict(m1, test)
pred
test$Accuracy <- pred$predict == test$W_4
1 - mean(test$Accuracy)
predictors_constant_46 <- c("REGNUM_BLOCK", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_46
highage_46_remove <- highage_46[, !(names(highage_46) %in% predictors_constant_46)]
highage_46_remove
splits <- h2o.splitFrame(highage_46_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_46_remove), response) # 전체컬럼
predictors
m1 <- h2o.deeplearning(   model_id = "dl_model_first",   training_frame = train,   validation_frame = valid, ## validation dataset: used for scoring and early stopping   x = predictors,   y = response,   #activation="Rectifier",  ## default   #hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each   epochs = 1,   variable_importances = T ## not enabled by default )
summary(m1)
pred <- h2o.predict(m1, test)
pred
test$Accuracy <- pred$predict == test$W_4
1 - mean(test$Accuracy)
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_26 <- read.table('highage_26.csv', header = TRUE)
highage_26 <- as.h2o(highage_26)
highage_26
splits <- h2o.splitFrame(highage_26, c(0.6, 0.2), seed = 1234) train <- h2o.assign(splits[[1]], "train.hex") # 60% valid <- h2o.assign(splits[[2]], "valid.hex") # 60% test <- h2o.assign(splits[[3]], "test.hex") # 60%
predictors <- setdiff(names(highage_26), response) # 전체컬럼
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_26), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) #
summary(rf1)
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_26 <- read.table('highage_26.csv', header = TRUE)
highage_26 <- as.h2o(highage_26)
highage_26
predictors_constant_26 <- c("REGNUM_BLOCK", "SEQ_NUM", "SEQ_", "W_33", "W_34", "W_24", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "WA_6", "W_3", "WA_7", "W_40", "W_30", "W_41", "W_31", "WA_1", "WA_4")
predictors_constant_26
highage_26_remove <- highage_26[, !(names(highage_26) %in% predictors_constant_26)]
highage_26_remove
splits <- h2o.splitFrame(highage_26_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_26_remove), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
finalRf_predictions <- h2o.predict(   object = rf1   , newdata = test)
finalRf_predictions
mean(finalRf_predictions$predict == test$W_4) ##1?? 왜 1인지 test set accuracy
test$Accuracy <- finalRf_predictions$predict == test$W_4
1 - mean(test$Accuracy)
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_36 <- read.table('highage_36.csv', header = TRUE)
highage_36 <- as.h2o(highage_36) highage_36
splits <- h2o.splitFrame(highage_36, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60% valid <- h2o.assign(splits[[2]], "valid.hex") # 60% test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_36), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_36 <- read.table('highage_36.csv', header = TRUE)
highage_36 <- as.h2o(highage_36) highage_36
predictors_constant_36 <- c("REGNUM_BLOCK", "BLOCK_NM", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_36
highage_36_remove <- highage_36[, !(names(highage_36) %in% predictors_constant_36)]
highage_36_remove
splits <- h2o.splitFrame(highage_36_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_36 <- read.table('highage_36.csv', header = TRUE)
highage_36 <- as.h2o(highage_36) highage_36
predictors_constant_36 <- c("SEQ_NUM", "SEQ_", "REGNUM_BLOCK", "BLOCK_NM", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_36
highage_36_remove <- highage_36[, !(names(highage_36) %in% predictors_constant_36)]
highage_36_remove
splits <- h2o.splitFrame(highage_36_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
splits <- h2o.splitFrame(highage_36_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
finalRf_predictions <- h2o.predict(   object = rf1   , newdata = test)
finalRf_predictions
mean(finalRf_predictions$predict == test$W_4) ##1?? 왜 1인지 test set accuracy
test$Accuracy <- finalRf_predictions$predict == test$W_4
1 - mean(test$Accuracy)
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_46 <- read.table('highage_36.csv', header = TRUE)
highage_46 <- as.h2o(highage_46) highage_46
splits <- h2o.splitFrame(highage_46, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_46), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_46 <- as.h2o(highage_46)
highage_46
highage_46 <- as.h2o(highage_46)
h2o.removeAll() ## clean slate - just in case the cluster was already running
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
highage_46 <- as.h2o(highage_46)
highage_46 <- read.table('highage_36.csv', header = TRUE)
highage_46 <- as.h2o(highage_46)
highage_46
predictors_constant_46 <- c("W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_46
highage_46_remove <- highage_46[, !(names(highage_46) %in% predictors_constant_46)]
highage_46_remove
splits <- h2o.splitFrame(highage_46_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_46_remove), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
h2o.removeAll() ## clean slate - just in case the cluster was already running
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
highage_46 <- read.table('highage_36.csv', header = TRUE)
highage_46 <- as.h2o(highage_46) highage_46
predictors_constant_46 <- c("REGNUM_BLOCK", "BLOCK_NM", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_46
highage_46_remove <- highage_46[, !(names(highage_46) %in% predictors_constant_46)]
highage_46_remove
## 고령자 46 splits <- h2o.splitFrame(highage_46_remove, c(0.6, 0.2), seed = 1234) train <- h2o.assign(splits[[1]], "train.hex") # 60% valid <- h2o.assign(splits[[2]], "valid.hex") # 60% test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_46_remove), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_46 <- read.table('highage_36.csv', header = TRUE)
highage_46 <- as.h2o(highage_46) highage_46
predictors_constant_46 <- c("SEQ_NUM", "SEQ_", "REGNUM_BLOCK", "BLOCK_NM", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_46
highage_46_remove <- highage_46[, !(names(highage_46) %in% predictors_constant_46)]
highage_46_remove
splits <- h2o.splitFrame(highage_46_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_46_remove), response) # 전체컬럼
predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
finalRf_predictions <- h2o.predict(   object = rf1   , newdata = test)
finalRf_predictions
mean(finalRf_predictions$predict == test$W_4) ##1?? 왜 1인지 test set accuracy
test$Accuracy <- finalRf_predictions$predict == test$W_4
1 - mean(test$Accuracy)
rm(list = ls())
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_46 <- read.table('highage_36.csv', header = TRUE)
highage_46 <- as.h2o(highage_46)
highage_46
predictors_constant_46 <- c("SEQ_NUM", "SEQ_", "REGNUM_BLOCK", "BLOCK_NM", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_46
highage_46_remove <- highage_46[, !(names(highage_46) %in% predictors_constant_46)]
highage_46_remove
splits <- h2o.splitFrame(highage_46_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_46_remove), response) # 전체컬럼 predictors
rf1 <- h2o.randomForest(## h2o.randomForest function   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "rf_covType_v1", ## name the model in H2O                                  ##   not required, but helps use Flow   ntrees = 200, ## use a maximum of 200 trees to create the                                  ##  random forest model. The default is 50.                                  ##  I have increased it because I will let                                   ##  the early stopping criteria decide when                                  ##  the random forest is sufficiently accurate   stopping_rounds = 2, ## Stop fitting new trees when the 2-tree                                  ##  average is within 0.001 (default) of                                   ##  the prior two 2-tree averages.                                  ##  Can be thought of as a convergence setting   score_each_iteration = T, ## Predict against training and validation for                                  ##  each tree. Default will skip several.   seed = 1000000) ## Set the random seed so that this can be
summary(rf1)
finalRf_predictions <- h2o.predict(   object = rf1   , newdata = test)
finalRf_predictions
mean(finalRf_predictions$predict == test$W_4) ##1?? 왜 1인지 test set accuracy
test$Accuracy <- finalRf_predictions$predict == test$W_4
1 - mean(test$Accuracy)
rm(list = ls())
rm(list = ls())
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_26 <- read.table('highage_26.csv', header = TRUE)
highage_26 <- as.h2o(highage_26)
highage_26
splits <- h2o.splitFrame(highage_26, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_26), response) # 전체컬럼
predictors
gbm1 <- h2o.gbm(   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "gbm_covType1", ## name the model in H2O   seed = 2000000) ## Set the random seed for reproducability
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_26 <- read.table('highage_26.csv', header = TRUE)
highage_26 <- as.h2o(highage_26)
highage_26
highage_26 <- as.h2o(highage_26)
highage_26
predictors_constant_26 <- c("REGNUM_BLOCK", "SEQ_NUM", "SEQ_", "W_33", "W_34", "W_24", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "WA_6", "W_3", "WA_7", "W_40", "W_30", "W_41", "W_31", "WA_1", "WA_4")
predictors_constant_26
highage_26_remove <- highage_26[, !(names(highage_26) %in% predictors_constant_26)]
highage_26_remove
splits <- h2o.splitFrame(highage_26_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_26_remove), response) # 전체컬럼
predictors
gbm1 <- h2o.gbm(   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "gbm_covType1", ## name the model in H2O   seed = 2000000) ## Set the random seed for reproducability
summary(gbm1)
summary(gbm1)
finalgbm_predictions <- h2o.predict(   object = gbm1   , newdata = test)
finalgbm_predictions
mean(finalgbm_predictions$predict == test$W_4) ##1?? 왜 1인지 test set accuracy
test$Accuracy <- finalRf_predictions$predict == test$W_4
test$Accuracy <- finalgbm_predictions$predict == test$W_4
1 - mean(test$Accuracy)
rm(list = ls())
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_36 <- read.table('highage_36.csv', header = TRUE)
highage_36 <- as.h2o(highage_36) highage_36
splits <- h2o.splitFrame(highage_36, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼 predictors
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼
predictors <- setdiff(names(highage_36), response) # 전체컬럼
predictors
gbm1 <- h2o.gbm(   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "gbm_covType1", ## name the model in H2O   seed = 2000000) ## Set the random seed for reproducability
rm(list = ls())
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_36 <- read.table('highage_36.csv', header = TRUE)
highage_36 <- as.h2o(highage_36) highage_36
predictors_constant_36 <- c("SEQ_NUM", "SEQ_", "REGNUM_BLOCK", "BLOCK_NM", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_36
highage_36_remove <- highage_36[, !(names(highage_36) %in% predictors_constant_36)]
highage_36_remove
splits <- h2o.splitFrame(highage_36_remove, c(0.6, 0.2), seed = 1234)
train <- h2o.assign(splits[[1]], "train.hex") # 60%
valid <- h2o.assign(splits[[2]], "valid.hex") # 60%
test <- h2o.assign(splits[[3]], "test.hex") # 60%
predictors <- setdiff(names(highage_36), response) # 전체컬럼
predictors
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_36_remove), response) # 전체컬럼
predictors
gbm1 <- h2o.gbm(   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "gbm_covType1", ## name the model in H2O   seed = 2000000) ## Set the random seed for reproducability
summary(gbm1)
, newdata = test)
finalgbm_predictions <- h2o.predict(   object = gbm1   , newdata = test)
finalgbm_predictions
mean(finalgbm_predictions$predict == test$W_4) ##1?? 왜 1인지 test set accuracy
test$Accuracy <- finalgbm_predictions$predict == test$W_4
1 - mean(test$Accuracy)
rm(list = ls())
library(h2o)
localH2O <- h2o.init(max_mem_size = '1g') ## using a max 1GB of RAM
h2o.removeAll() ## clean slate - just in case the cluster was already running
highage_46 <- read.table('highage_36.csv', header = TRUE)
highage_46 <- as.h2o(highage_46) highage_46
predictors_constant_46 <- c("SEQ_NUM", "SEQ_", "REGNUM_BLOCK", "BLOCK_NM", "W_34", "W_36", "W_15", "W_38", "W_17", "W_39", "WA_5", "W_3", "WA_7", "W_40", "W_30", "W_41", "WA_1", "WA_4")
predictors_constant_46
highage_46_remove <- highage_46[, !(names(highage_46) %in% predictors_constant_46)]
highage_46_remove
splits <- h2o.splitFrame(highage_46_remove, c(0.6, 0.2), seed = 1234) train <- h2o.assign(splits[[1]], "train.hex") # 60% valid <- h2o.assign(splits[[2]], "valid.hex") # 60% test <- h2o.assign(splits[[3]], "test.hex") # 60%
response = 'W_4' # 당첨/탈락 여부 필드
predictors <- setdiff(names(highage_46_remove), response) # 전체컬럼
predictors
gbm1 <- h2o.gbm(   training_frame = train, ## the H2O frame for training   validation_frame = valid, ## the H2O frame for validation (not required)   x = predictors, ## the predictor columns, by column index   y = response, ## the target index (what we are predicting)   model_id = "gbm_covType1", ## name the model in H2O   seed = 2000000) ## Set the random seed for reproducability
summary(gbm1)
summary(gbm1)
finalgbm_predictions <- h2o.predict(   object = gbm1   , newdata = test)
finalgbm_predictions mean(finalgbm_predictions$predict == test$W_4) ##1?? 왜 1인지 test set accuracy test$Accuracy <- finalgbm_predictions$predict == test$W_4 1 - mean(test$Accuracy)
?which
data(iris)
summary(iris)
head(iris)
which(iris$Species == "setosa")
iris2 <- iris
which(iris2$Species == "setosa")
iris2$Species[which(iris2$Species == "setosa")]
iris2$Species[which(iris2$Species == "setosa")] <- 1
iris2$Species[which(iris2$Species == "setosa"),] <- 1
iris2$Species[,which(iris2$Species == "setosa")] <- 1
iris2[which(iris2$Species == "setosa")] <- 1
iris2
iris2[which(iris2$Species == "virginica"),] <- 1
iris2[,which(iris2$Species == "virginica")] <- 1
iris2
iris2 <- iris
which(iris2$Species == "setosa")
iris2[which(iris2$Species == "setosa"),]
iris2[,which(iris2$Species == "setosa")]
iris2$Species[which(iris2$Species == "setosa")]
iris2$Species <- iris2[iris2$Species[which(iris2$Species == "setosa")]]
iris2
head(iris2)
iris2 <- iris
iris2[iris2$Species[which(iris2$Species == "setosa")]]
iris2[,iris2$Species[which(iris2$Species == "setosa")]]
iris2[iris2$Species[which(iris2$Species == "setosa")],]
iris2[iris2[which(iris2$Species == "setosa")],]
iris2[,iris2[which(iris2$Species == "setosa")]]
iris2[iris2[which(iris2$Species == "setosa")]]
which(iris2$Species == "setosa")
iris2[which(iris2$Species == "setosa"),] <- 1
iris2[which(iris2$Species == "setosa")] <- 1
head(iris2)
iris2 <- iris
which(iris2$Species == "setosa")
iris2[which(iris2$Species == "setosa")]
iris2[which(iris2$Species == "setosa"),]
iris2[which(iris2$Species == "setosa"),Species]
iris2[which(iris2$Species == "setosa"),4]
iris2[which(iris2$Species == "setosa"),5]
iris2[which(iris2$Species == "setosa"),5] <- 1
iris2$Species == "setosa"
iris2[iris2$Species == "setosa"]
iris2[iris2$Species == "setosa",]
iris2[iris2$Species == "setosa",]
iris2[,iris2$Species == "setosa"]
iris2[,iris2$Species == "setosa"]
iris2[iris2$Species == "setosa",]
iris2[iris2$Species == "setosa"]
iris2 <- iris
iris2[iris2$Species == "setosa"]
iris2[iris2$Species == "setosa",]
iris2[,iris2$Species == "setosa"]
iris2[iris2$Species == "setosa",]
iris2[iris2$Species == "setosa",5]
iris2[iris2=="setosa"]
iris2[iris2=="setosa"] <- 1
iris2[iris2=="setosa",] <- 1
iris2[,iris2=="setosa"] <- 1
iris2[iris2=="setosa"] <- 1
iris2
iris2 <- iris
factor(iris2$Species, levels=c(1:3), ordered=T, labels = c(1,2,3))
iris2[which(iris2$Species == "setosa")]
iris2[which(iris2$Species == "setosa"),]
iris2[which(iris2$Species == "setosa"),5]
?rep
iris2[which(iris2$Species == "setosa"),5] <- rep(1, length(which(iris2$Species == "setosa")))
iris2[which(iris2$Species == "setosa"),5]
iris2 <- iris
iris2[which(iris2$Species == "setosa"),5]
iris2[which(iris2$Species == "setosa")]
which(iris2$Species == "setosa")
which(iris2$Species == "setosa") <- 1
iris2[,which(iris2$Species == "setosa")]
iris2[which(iris2$Species == "setosa")]
iris2[which(iris2$Species == "setosa"),]
iris2[which(iris2$Species == "setosa"),5]
iris2[which(iris2$Species == "setosa"),5] <- "1"
iris2$Species[iris2$Species == "setosa"]
iris2 <- iris
iris2$Species[iris2$Species == "setosa"]
iris2$Species[iris2$Species == "setosa"] <- 1
library(plyr)
revalue(iris2$Species , c("setosa", 1))
revalue(iris2$Species , c("setosa"= 1))
iris2
head(iris2)
levels(iris2$Species)[levels(iris2$Species)=="setosa"] <- "b"
head(iris2)
levels(iris2$Species)[levels(iris2$Species)=="setosa"] <- "b"
iris2
head(iris2)
iris2$Species[which(iris2$Species=="setosa")]<-"b"
iris2
head(iris2)
iris2 <- iris
iris2
head(iris2)
iris2$Species[which(iris2$Species=="setosa")]<-"b"
iris2
head(iris2)
iris2 <- iris
unique(iris2$Species)
factor(iris2$Species, levels=c("setosa", "versicolor", "virginica"), ordered=T, labels = c(1,2,3))
